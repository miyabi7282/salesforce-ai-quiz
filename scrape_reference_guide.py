import asyncio
from playwright.async_api import async_playwright
import yaml
from urllib.parse import urljoin
import json

# --- è¨­å®šé …ç›® ---
START_URL = "https://developer.salesforce.com/docs/data/data-cloud-ref/guide/c360a-api-intro-cdpapis.htm"
OUTPUT_FILE = "salesforce_data_cloud_reference_guide.yaml"
# â˜…â˜…â˜… ä¸¦åˆ—å‡¦ç†ã®åŒæ™‚å®Ÿè¡Œæ•°ã‚’è¨­å®š â˜…â˜…â˜…
MAX_CONCURRENT_TASKS = 5

# --- CSSã‚»ãƒ¬ã‚¯ã‚¿ ---
MAIN_CONTENT_HOST_SELECTOR = "doc-content-layout"
COOKIE_BUTTON_SELECTOR = "#onetrust-accept-btn-handler"

async def accept_cookies_if_present(page):
    """ã‚¯ãƒƒã‚­ãƒ¼åŒæ„ãƒãƒŠãƒ¼ãŒã‚ã‚Œã°ã‚¯ãƒªãƒƒã‚¯ã—ã¦é–‰ã˜ã‚‹"""
    try:
        await page.locator(COOKIE_BUTTON_SELECTOR).click(timeout=10000)
        print("âœ” ã‚¯ãƒƒã‚­ãƒ¼åŒæ„ãƒãƒŠãƒ¼ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
        await page.wait_for_timeout(1000)
    except Exception:
        print("â„¹ ã‚¯ãƒƒã‚­ãƒ¼ãƒãƒŠãƒ¼ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

def extract_links_from_json(json_data, base_url):
    """å†å¸°çš„ã«JSONã‚’æ¢ç´¢ã—ã€å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹"""
    links = set()
    for item in json_data:
        if 'link' in item and 'href' in item['link']:
            links.add(urljoin(base_url, item['link']['href']))
        if 'children' in item and item['children']:
            links.update(extract_links_from_json(item['children'], base_url))
    return links

async def get_all_article_links_from_json(page):
    """ãƒšãƒ¼ã‚¸ã®å±æ€§ã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸJSONã‹ã‚‰å…¨ãƒªãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹"""
    print("ğŸ”— åŸ‹ã‚è¾¼ã¿JSONã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºä¸­...")
    sidebar_content_str = await page.locator(MAIN_CONTENT_HOST_SELECTOR).get_attribute("sidebar-content")
    if not sidebar_content_str:
        raise Exception("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®JSONãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    sidebar_data = json.loads(sidebar_content_str)
    links = extract_links_from_json(sidebar_data, page.url)
    unique_links = sorted(list(links))
    print(f"âœ” {len(unique_links)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return unique_links

# â˜…â˜…â˜… å‡¦ç†ã®å˜ä½ã¨ãªã‚‹é–¢æ•°ã‚’ä¿®æ­£ â˜…â˜…â˜…
async def scrape_single_article(context, url, index, total):
    """
    æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ã¦å˜ä¸€ã®è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€çµæœã‚’è¿”ã™ã€‚
    contextã‚’å¼•æ•°ã«å–ã‚‹ã“ã¨ã§ã€ä¸¦åˆ—å‡¦ç†ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚
    """
    print(f"[{index}/{total}] å–å¾—é–‹å§‹: {url.split('/')[-1]}")
    page = await context.new_page()
    try:
        await page.goto(url, wait_until="load", timeout=60000)
        
        host_locator = page.locator(MAIN_CONTENT_HOST_SELECTOR)
        await host_locator.wait_for(timeout=30000)
        
        title = await page.locator("h1").inner_text()
        content = await host_locator.inner_text()
        
        print(f"[{index}/{total}] âœ” å–å¾—å®Œäº†: {title[:30]}...")
        return {"url": url, "title": title.strip(), "content": content.strip()}
    except Exception as e:
        print(f"[{index}/{total}] âŒ è¨˜äº‹å–å¾—å¤±æ•—: {url.split('/')[-1]}\n   ç†ç”±: {str(e).splitlines()[0]}")
        return None
    finally:
        # å‡¦ç†ãŒçµ‚ã‚ã£ãŸã‚‰å¿…ãšãƒšãƒ¼ã‚¸ã‚’é–‰ã˜ã‚‹
        await page.close()

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        try:
            print(f"\nğŸ“˜ èµ·ç‚¹ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹: {START_URL}")
            await page.goto(START_URL, wait_until="load", timeout=90000)
            
            # â˜…â˜…â˜… ã‚¯ãƒƒã‚­ãƒ¼å‡¦ç†ã¯ã“ã“ã§ä¸€åº¦ã ã‘å®Ÿè¡Œ â˜…â˜…â˜…
            await accept_cookies_if_present(page)

            article_links = await get_all_article_links_from_json(page)
            # ãƒªãƒ³ã‚¯å–å¾—ãŒçµ‚ã‚ã£ãŸã‚‰ã€æœ€åˆã®ãƒšãƒ¼ã‚¸ã¯ã‚‚ã†ä¸è¦
            await page.close()

            print(f"\nğŸ“° è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åé›†ä¸­... (æœ€å¤§{MAX_CONCURRENT_TASKS}ä»¶ã®ä¸¦åˆ—å‡¦ç†)")
            
            # â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒä¸¦åˆ—å‡¦ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ â˜…â˜…â˜…
            tasks = []
            for i, url in enumerate(article_links, 1):
                # å„è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã€Œã‚¿ã‚¹ã‚¯ã€ã¨ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                task = scrape_single_article(context, url, i, len(article_links))
                tasks.append(task)
            
            # asyncio.gatherã§ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            results = await asyncio.gather(*tasks)
            
            # å¤±æ•—ã—ãŸçµæœ(None)ã‚’é™¤å¤–
            all_articles = [res for res in results if res is not None]

            if all_articles:
                with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                    yaml.dump(all_articles, f, allow_unicode=True, sort_keys=False, indent=2)
                print(f"\nâœ… å…¨{len(all_articles)} ä»¶ã®è¨˜äº‹ã‚’ {OUTPUT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            else:
                print("âŒ æœ‰åŠ¹ãªè¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        except Exception as e:
            print(f"\nâŒè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ã€ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ãŒã¾ã é–‹ã„ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’è©¦ã¿ã‚‹
            if not page.is_closed():
                await page.screenshot(path="playwright_fatal_error_ref_guide.png")
                print("ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
        finally:
            await browser.close()

if __name__ == "__main__":
    asyncio.run(main())