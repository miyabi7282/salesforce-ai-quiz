import asyncio
from playwright.async_api import async_playwright
import yaml
from urllib.parse import urljoin

# --- è¨­å®šé …ç›® ---
START_URL = "https://help.salesforce.com/s/articleView?id=data.c360_a_product_considerations.htm&type=5&language=ja"
OUTPUT_FILE = "salesforce_help_articles.yaml"
# â˜…â˜…â˜… ãŠå®¢æ§˜ã®ç™ºè¦‹ã«ã‚ˆã‚‹ã€çœŸã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚»ãƒ¬ã‚¯ã‚¿ â˜…â˜…â˜…
# ã‚¯ãƒ©ã‚¹åãŒå‹•çš„ã«å¤‰ã‚ã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€éƒ¨åˆ†ä¸€è‡´ã§å …ç‰¢ã«æŒ‡å®š
SIDEBAR_SELECTOR = "div[class*='table-of-content']" 
CONTENT_SELECTOR = "div.slds-text-longform"

async def accept_cookies_if_present(page):
    """ã‚¯ãƒƒã‚­ãƒ¼åŒæ„ãƒãƒŠãƒ¼ãŒã‚ã‚Œã°ã‚¯ãƒªãƒƒã‚¯ã—ã¦é–‰ã˜ã‚‹"""
    try:
        await page.locator("#onetrust-accept-btn-handler").click(timeout=8000)
        print("âœ” ã‚¯ãƒƒã‚­ãƒ¼åŒæ„ãƒãƒŠãƒ¼ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
        await page.wait_for_timeout(1000)
    except Exception:
        print("â„¹ ã‚¯ãƒƒã‚­ãƒ¼ãƒãƒŠãƒ¼ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

async def expand_all_sidebar_items(page):
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãªãŒã‚‰ã€è¦‹ãˆã‚‹ç¯„å›²ã®ãƒœã‚¿ãƒ³ã‚’ç¹°ã‚Šè¿”ã—ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹"""
    print("ğŸ“‚ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å…¨é …ç›®ã‚’å±•é–‹ã—ã¾ã™...")
    
    for i in range(10):
        # â˜…â˜…â˜… æ­£ã—ã„ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨ â˜…â˜…â˜…
        closed_buttons_locator = page.locator(f"{SIDEBAR_SELECTOR} button[aria-expanded='false']")
        
        clicked_in_this_pass = 0
        for button in await closed_buttons_locator.all():
            try:
                if await button.is_visible(timeout=500):
                    await button.scroll_into_view_if_needed(timeout=3000)
                    await button.click(timeout=1000)
                    clicked_in_this_pass += 1
                    await page.wait_for_timeout(50) 
            except Exception:
                pass
        
        if clicked_in_this_pass == 0:
            print(f"  â†ª ãƒ‘ã‚¹ {i+1}: ã“ã‚Œä»¥ä¸Šå±•é–‹ã§ãã‚‹é …ç›®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            break
        else:
            print(f"  â†ª ãƒ‘ã‚¹ {i+1}: {clicked_in_this_pass}å€‹ã®é …ç›®ã‚’å±•é–‹ã—ã¾ã—ãŸã€‚")
    
    print("âœ” å…¨ã¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼å±•é–‹ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    await page.wait_for_timeout(3000)

async def get_all_article_links(page):
    """locatorã‚’ä½¿ã£ã¦ã‚µã‚¤ãƒ‰ãƒãƒ¼å†…ã®ãƒªãƒ³ã‚¯ã‚’å…¨ã¦å–å¾—ã™ã‚‹"""
    print("ğŸ”— ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºä¸­...")
    
    # â˜…â˜…â˜… æ­£ã—ã„ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨ â˜…â˜…â˜…
    link_selector = f"{SIDEBAR_SELECTOR} a[href*='/s/articleView?id=']"
    
    await page.locator(link_selector).first.wait_for(timeout=20000)
    
    links_locator = page.locator(link_selector)
    count = await links_locator.count()
    
    if count == 0:
        raise Exception("ãƒªãƒ³ã‚¯ãŒ1ä»¶ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    all_links = []
    for i in range(count):
        href = await links_locator.nth(i).get_attribute("href")
        if href:
            full_url = urljoin(page.url, href)
            all_links.append(full_url)

    unique_links = sorted(list(set(all_links)))
    print(f"âœ” {len(unique_links)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return unique_links

async def scrape_article(page, url):
    """locatorã‚’ä½¿ã£ã¦è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        await page.goto(url, wait_until="load", timeout=60000)
        
        content_locator = page.locator(CONTENT_SELECTOR)
        await content_locator.wait_for(timeout=30000)
        
        title = await page.title()
        content = await content_locator.inner_text()
        
        return {
            "url": url,
            "title": title.strip(),
            "content": content.strip()
        }
    except Exception as e:
        print(f"âŒ è¨˜äº‹å–å¾—å¤±æ•—: {url}\n   ç†ç”±: {str(e).splitlines()[0]}")
        return None

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()

        try:
            print(f"\nğŸ“˜ èµ·ç‚¹ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹: {START_URL}")
            await page.goto(START_URL, wait_until="load", timeout=90000)
            await accept_cookies_if_present(page)

            # â˜…â˜…â˜… æ­£ã—ã„ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ä½¿ç”¨ â˜…â˜…â˜…
            await page.wait_for_selector(SIDEBAR_SELECTOR, timeout=30000)
            print("âœ” ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")

            await expand_all_sidebar_items(page)
            
            article_links = await get_all_article_links(page)

            print("\nğŸ“° è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åé›†ä¸­...")
            all_articles = []
            for i, url in enumerate(article_links, 1):
                print(f"[{i + 1}/{len(article_links)}] å–å¾—ä¸­...")
                article = await scrape_article(page, url)
                if article:
                    all_articles.append(article)
                await page.wait_for_timeout(1000)

            if all_articles:
                with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                    yaml.dump(all_articles, f, allow_unicode=True, sort_keys=False, indent=2)
                print(f"\nâœ… {len(all_articles)} ä»¶ã®è¨˜äº‹ã‚’ {OUTPUT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
            else:
                print("âŒ æœ‰åŠ¹ãªè¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        except Exception as e:
            print(f"\nâŒè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            await page.screenshot(path="playwright_fatal_error.png")
            print("ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ 'playwright_fatal_error.png' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        finally:
            await browser.close()

if __name__ == "__main__":
    asyncio.run(main())