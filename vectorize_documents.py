import os
import yaml
import faiss
import numpy as np
import google.generativeai as genai
from langchain_text_splitters import RecursiveCharacterTextSplitter
import pickle
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
import time
import re

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- è¨­å®šé …ç›® ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# å…¥åŠ›ã¨ãªã‚‹YAMLãƒ•ã‚¡ã‚¤ãƒ«
INPUT_FILES = [
    "salesforce_help_articles.yaml",
    "salesforce_guides_consolidated.yaml",
    "salesforce_data_cloud_reference_guide.yaml",
    "salesforce_data_cloud_developer_guide.yaml"
]

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
FAISS_INDEX_FILE = "salesforce_docs.faiss"
TEXT_CHUNKS_FILE = "salesforce_docs_chunks.pkl"
BM25_INDEX_FILE = "salesforce_docs.bm25"

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã®è¨­å®š
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 100

def load_documents_from_files(filenames):
    """è¤‡æ•°ã®YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    all_docs = []
    print("--- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿é–‹å§‹ ---")
    for filename in filenames:
        if not os.path.exists(filename):
            print(f"è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        
        with open(filename, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            if isinstance(data, list):
                valid_docs = [doc for doc in data if doc and doc.get('content') and isinstance(doc.get('content'), str)]
                all_docs.extend(valid_docs)
                print(f"âœ” '{filename}' ã‹ã‚‰ {len(valid_docs)} ä»¶ã®æœ‰åŠ¹ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    print(f"âœ” åˆè¨ˆ {len(all_docs)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿å®Œäº†ã€‚")
    return all_docs

def split_documents_into_chunks(documents):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿ï¼†åˆ†å‰²æˆ¦ç•¥æ”¹å–„ï¼‰"""
    print("\n--- ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’é–‹å§‹ ---")
    
    # æ”¹è¡Œã‚„å¥èª­ç‚¹ã‚’å„ªå…ˆçš„ã«åŒºåˆ‡ã‚Šæ–‡å­—ã¨ã—ã¦ä½¿ã†ã‚ˆã†ã«è¨­å®š
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""] # å„ªå…ˆé †ä½ãŒé«˜ã„åŒºåˆ‡ã‚Šæ–‡å­—
    )
    
    chunks_with_metadata = []
    for doc in documents:
        content = doc.get('content', '')
        if not content or not isinstance(content, str):
            continue
            
        split_texts = text_splitter.split_text(content)
        
        for text in split_texts:
            # å„ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­ã«ã€å‡ºå…¸ã¨ã‚¿ã‚¤ãƒˆãƒ«ã®æƒ…å ±ã‚’è¿½åŠ 
            source_info = doc.get('url') or doc.get('source_document', 'N/A')
            title_info = doc.get('title', 'N/A')
            
            # ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆæœ¬ä½“
            chunk_text = f"å‡ºå…¸: {source_info}\nã‚¿ã‚¤ãƒˆãƒ«: {title_info}\n\n{text}"
            
            chunks_with_metadata.append({
                "text": chunk_text, # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒåŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
                "source": source_info,
                "title": title_info
            })
            
    print(f"âœ” {len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ {len(chunks_with_metadata)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")
    return chunks_with_metadata

def vectorize_chunks(chunks):
    """Gemini APIã‚’ä½¿ã£ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹"""
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_GEMINI_API_KEY":
        print("\nâ˜…â˜…â˜… ã‚¨ãƒ©ãƒ¼: Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚â˜…â˜…â˜…")
        return None, None

    print("\n--- ãƒãƒ£ãƒ³ã‚¯ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’é–‹å§‹ ---")
    print("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: text-embedding-004")
    
    genai.configure(api_key=GEMINI_API_KEY)
    
    vectors = []
    batch_size = 100
    for i in range(0, len(chunks), batch_size):
        # ãƒãƒ£ãƒ³ã‚¯ã®è¾æ›¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã ã‘ã‚’æŠ½å‡º
        batch_texts = [chunk["text"] for chunk in chunks[i:i+batch_size]]
        
        try:
            time.sleep(1) # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=batch_texts,
                task_type="RETRIEVAL_DOCUMENT"
            )
            vectors.extend(result['embedding'])
            progress = min(i + batch_size, len(chunks))
            print(f"  - é€²è¡ŒçŠ¶æ³: {progress} / {len(chunks)} ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¸ˆã¿...")
        except Exception as e:
            print(f"âœ– ãƒãƒƒãƒå‡¦ç†ä¸­ã«APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (ä»¶å: {i}ï½{i+batch_size}): {e}")
            vectors.extend([None] * len(batch_texts))

    print(f"âœ” ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†å®Œäº†ã€‚")
    
    valid_vectors = []
    valid_chunks = []
    for chunk, vector in zip(chunks, vectors):
        if vector is not None:
            valid_chunks.append(chunk)
            valid_vectors.append(vector)

    if not valid_vectors:
        return None, None
        
    return np.array(valid_vectors).astype('float32'), valid_chunks


def create_and_save_faiss_index(vectors, index_path):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã€ä¿å­˜ã™ã‚‹"""
    if vectors is None or len(vectors) == 0:
        print("âœ– ãƒ™ã‚¯ãƒˆãƒ«ãŒç©ºã®ãŸã‚ã€Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚")
        return False
        
    print("\n--- Faissãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã¨ä¿å­˜ã‚’é–‹å§‹ ---")
    dimension = vectors.shape[1]
    
    if faiss.get_num_gpus() > 0:
        print(f"âœ” {faiss.get_num_gpus()}å€‹ã®GPUã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚GPUç‰ˆFaissã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        res = faiss.StandardGpuResources()
        index_gpu = faiss.GpuIndexFlatL2(res, dimension)
        index_gpu.add(vectors)
        print(f"  - GPUä¸Šã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†ã€‚CPUã«è»¢é€ã—ã¦ä¿å­˜ã—ã¾ã™...")
        index_cpu = faiss.index_gpu_to_cpu(index_gpu)
        faiss.write_index(index_cpu, index_path)
    else:
        print("â„¹ GPUãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CPUç‰ˆFaissã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        index = faiss.IndexFlatL2(dimension)
        index.add(vectors)
        faiss.write_index(index, index_path)

    print(f"âœ” Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{index_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return True

def create_and_save_bm25_index(chunks, index_path):
    """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã€ä¿å­˜ã™ã‚‹"""
    print("\n--- BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã¨ä¿å­˜ã‚’é–‹å§‹ ---")
    
    def simple_tokenizer(text):
        return re.findall(r'[A-Za-z0-9]+|[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+', text.lower())

    # BM25ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã«ã¯ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒåŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ãªã„å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ã†æ–¹ãŒè‰¯ã„å ´åˆãŒã‚ã‚‹
    # ã“ã“ã§ã¯ã€åŸ‹ã‚è¾¼ã¿æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾ä½¿ã†
    tokenized_corpus = [simple_tokenizer(chunk["text"]) for chunk in chunks]
    
    bm25 = BM25Okapi(tokenized_corpus)
    
    with open(index_path, 'wb') as f:
        pickle.dump(bm25, f)
        
    print(f"âœ” BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ '{index_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def save_chunks(chunks, chunks_path):
    """ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹"""
    with open(chunks_path, 'wb') as f:
        pickle.dump(chunks, f)
    print(f"âœ” ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ '{chunks_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    docs = load_documents_from_files(INPUT_FILES)
    
    if docs:
        chunks = split_documents_into_chunks(docs)
        if chunks:
            vectors, valid_chunks = vectorize_chunks(chunks)
            
            if vectors is not None and len(vectors) > 0:
                create_and_save_bm25_index(valid_chunks, BM25_INDEX_FILE)
                if create_and_save_faiss_index(vectors, FAISS_INDEX_FILE):
                    save_chunks(valid_chunks, TEXT_CHUNKS_FILE)
                    print("\nğŸ‰ å…¨ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼ ğŸ‰")
                else:
                    print("\nâœ– Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆã¾ãŸã¯ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                print("\nâœ– æœ‰åŠ¹ãªãƒ™ã‚¯ãƒˆãƒ«ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        else:
            print("\nâœ– ãƒãƒ£ãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")